{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import classifiers as cl\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "import geopandas as gpd\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2014 = pd.read_csv('clean_data/data_2014.csv')\n",
    "data_2015 = pd.read_csv('clean_data/data_2015.csv')\n",
    "data_2016 = pd.read_csv('clean_data/data_2016.csv')\n",
    "data_2017 = pd.read_csv('clean_data/data_2017.csv')\n",
    "\n",
    "data_2014 = data_2014.fillna(0).drop('S0101_C01_001E', axis=1)\n",
    "data_2015 = data_2015.fillna(0).drop('S0101_C01_001E', axis=1)\n",
    "data_2016 = data_2016.fillna(0).drop('S0101_C01_001E', axis=1)\n",
    "data_2017 = data_2017.fillna(0).drop('S0101_C01_001E', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits={\n",
    "    'split_1':{\n",
    "        'x_train':cl.standardrize(data_2014.iloc[:,1:]),\n",
    "        'x_test':cl.standardrize(data_2015.iloc[:,1:]),\n",
    "        'y_train':data_2014['risk'],\n",
    "        'y_test':data_2015['risk']\n",
    "    },\n",
    "    'split_2':{\n",
    "        'x_train':cl.standardrize(data_2014.iloc[:,1:].append(data_2015.iloc[:,1:])),\n",
    "        'x_test':cl.standardrize(data_2016.iloc[:,1:]),\n",
    "        'y_train':data_2014['risk'].append(data_2015['risk']),\n",
    "        'y_test':data_2016['risk']\n",
    "    },\n",
    "        'split_3':{\n",
    "        'x_train':cl.standardrize(data_2014.iloc[:,1:].append(data_2015.iloc[:,1:]).append(data_2016.iloc[:,1:])),\n",
    "        'x_test':cl.standardrize(data_2017.iloc[:,1:]),\n",
    "        'y_train':data_2014['risk'].append(data_2015['risk']).append(data_2016['risk']),\n",
    "        'y_test':data_2016['risk']\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating param grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_params = {\n",
    "    'penalty':['l2'],\n",
    "    'C':[0.1, 0.5, 1.0],\n",
    "    'random_state':[42],\n",
    "    'max_iter':[100, 200, 300, 500],\n",
    "    'solver': ['lbfgs']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_params = {\n",
    "    'C':[0.1, 0.5, 1.0],\n",
    "    'kernel':['linear', 'poly', 'rbf'],\n",
    "    'gamma':['auto', 'scale'],\n",
    "    'max_iter':[100, 200, 300, 500, 1000],\n",
    "    'random_state':[42]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "boost_params ={\n",
    "    'n_estimators':[30, 50, 100, 150, 200],\n",
    "    'learning_rate':[0.1, 0.5, 0.7, 1.0],\n",
    "    'random_state':[42]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_params = {\n",
    "    'n_estimators':[30, 50, 100, 150, 200],\n",
    "    'criterion':['gini', 'entropy'],\n",
    "    'random_state':[42]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_grid = list(ParameterGrid(logistic_params))\n",
    "svm_grid = list(ParameterGrid(svm_params))\n",
    "boost_grid = list(ParameterGrid(boost_params))\n",
    "rf_grid = list(ParameterGrid(rf_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing different models for different splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_models = {\n",
    "    'logistic':{'model':LogisticRegression, 'grid':logistic_grid},\n",
    "    'boost':{'model':AdaBoostClassifier, 'grid':boost_grid},\n",
    "    'rf':{'model':RandomForestClassifier, 'grid':rf_grid}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "split1_results = cl.different_models(diff_models, splits['split_1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "split2_results = cl.different_models(diff_models, splits['split_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "split3_results = cl.different_models(diff_models, splits['split_3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>AUC</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>&lt;class 'sklearn.ensemble.forest.RandomForestClassifier'&gt;.- criterion: entropy, n_estimators: 200, random_state: 42</td>\n",
       "      <td>0.766082</td>\n",
       "      <td>0.847220</td>\n",
       "      <td>0.782407</td>\n",
       "      <td>0.760322</td>\n",
       "      <td>0.812546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>&lt;class 'sklearn.ensemble.forest.RandomForestClassifier'&gt;.- criterion: gini, n_estimators: 200, random_state: 42</td>\n",
       "      <td>0.765664</td>\n",
       "      <td>0.837395</td>\n",
       "      <td>0.787547</td>\n",
       "      <td>0.758589</td>\n",
       "      <td>0.810606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>&lt;class 'sklearn.ensemble.forest.RandomForestClassifier'&gt;.- criterion: entropy, n_estimators: 150, random_state: 42</td>\n",
       "      <td>0.761487</td>\n",
       "      <td>0.842515</td>\n",
       "      <td>0.779462</td>\n",
       "      <td>0.755344</td>\n",
       "      <td>0.808717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>&lt;class 'sklearn.ensemble.forest.RandomForestClassifier'&gt;.- criterion: gini, n_estimators: 100, random_state: 42</td>\n",
       "      <td>0.761905</td>\n",
       "      <td>0.835952</td>\n",
       "      <td>0.783256</td>\n",
       "      <td>0.754496</td>\n",
       "      <td>0.807827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>&lt;class 'sklearn.ensemble.forest.RandomForestClassifier'&gt;.- criterion: gini, n_estimators: 150, random_state: 42</td>\n",
       "      <td>0.760234</td>\n",
       "      <td>0.835300</td>\n",
       "      <td>0.782067</td>\n",
       "      <td>0.753085</td>\n",
       "      <td>0.806658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>&lt;class 'sklearn.ensemble.forest.RandomForestClassifier'&gt;.- criterion: entropy, n_estimators: 50, random_state: 42</td>\n",
       "      <td>0.759398</td>\n",
       "      <td>0.834925</td>\n",
       "      <td>0.780882</td>\n",
       "      <td>0.752806</td>\n",
       "      <td>0.805843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>&lt;class 'sklearn.linear_model.logistic.LogisticRegression'&gt;.- C: 0.1, max_iter: 200, penalty: l2, random_state: 42, solver: lbfgs</td>\n",
       "      <td>0.756475</td>\n",
       "      <td>0.852606</td>\n",
       "      <td>0.766587</td>\n",
       "      <td>0.751663</td>\n",
       "      <td>0.806864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>&lt;class 'sklearn.linear_model.logistic.LogisticRegression'&gt;.- C: 0.1, max_iter: 100, penalty: l2, random_state: 42, solver: lbfgs</td>\n",
       "      <td>0.756475</td>\n",
       "      <td>0.852606</td>\n",
       "      <td>0.766587</td>\n",
       "      <td>0.751663</td>\n",
       "      <td>0.806864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>&lt;class 'sklearn.linear_model.logistic.LogisticRegression'&gt;.- C: 0.1, max_iter: 500, penalty: l2, random_state: 42, solver: lbfgs</td>\n",
       "      <td>0.756475</td>\n",
       "      <td>0.852606</td>\n",
       "      <td>0.766587</td>\n",
       "      <td>0.751663</td>\n",
       "      <td>0.806864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>&lt;class 'sklearn.linear_model.logistic.LogisticRegression'&gt;.- C: 0.1, max_iter: 300, penalty: l2, random_state: 42, solver: lbfgs</td>\n",
       "      <td>0.756475</td>\n",
       "      <td>0.852606</td>\n",
       "      <td>0.766587</td>\n",
       "      <td>0.751663</td>\n",
       "      <td>0.806864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>&lt;class 'sklearn.ensemble.forest.RandomForestClassifier'&gt;.- criterion: gini, n_estimators: 50, random_state: 42</td>\n",
       "      <td>0.758981</td>\n",
       "      <td>0.827710</td>\n",
       "      <td>0.784440</td>\n",
       "      <td>0.750784</td>\n",
       "      <td>0.804483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>&lt;class 'sklearn.ensemble.forest.RandomForestClassifier'&gt;.- criterion: entropy, n_estimators: 100, random_state: 42</td>\n",
       "      <td>0.756475</td>\n",
       "      <td>0.836229</td>\n",
       "      <td>0.776360</td>\n",
       "      <td>0.749841</td>\n",
       "      <td>0.804170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>&lt;class 'sklearn.linear_model.logistic.LogisticRegression'&gt;.- C: 0.5, max_iter: 100, penalty: l2, random_state: 42, solver: lbfgs</td>\n",
       "      <td>0.752715</td>\n",
       "      <td>0.846666</td>\n",
       "      <td>0.765543</td>\n",
       "      <td>0.747278</td>\n",
       "      <td>0.803440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>&lt;class 'sklearn.linear_model.logistic.LogisticRegression'&gt;.- C: 0.5, max_iter: 300, penalty: l2, random_state: 42, solver: lbfgs</td>\n",
       "      <td>0.752715</td>\n",
       "      <td>0.846666</td>\n",
       "      <td>0.765543</td>\n",
       "      <td>0.747278</td>\n",
       "      <td>0.803440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>&lt;class 'sklearn.linear_model.logistic.LogisticRegression'&gt;.- C: 0.5, max_iter: 500, penalty: l2, random_state: 42, solver: lbfgs</td>\n",
       "      <td>0.752715</td>\n",
       "      <td>0.846666</td>\n",
       "      <td>0.765543</td>\n",
       "      <td>0.747278</td>\n",
       "      <td>0.803440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>&lt;class 'sklearn.linear_model.logistic.LogisticRegression'&gt;.- C: 0.5, max_iter: 200, penalty: l2, random_state: 42, solver: lbfgs</td>\n",
       "      <td>0.752715</td>\n",
       "      <td>0.846666</td>\n",
       "      <td>0.765543</td>\n",
       "      <td>0.747278</td>\n",
       "      <td>0.803440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>&lt;class 'sklearn.linear_model.logistic.LogisticRegression'&gt;.- C: 1.0, max_iter: 300, penalty: l2, random_state: 42, solver: lbfgs</td>\n",
       "      <td>0.752297</td>\n",
       "      <td>0.845362</td>\n",
       "      <td>0.765777</td>\n",
       "      <td>0.746725</td>\n",
       "      <td>0.802947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>&lt;class 'sklearn.linear_model.logistic.LogisticRegression'&gt;.- C: 1.0, max_iter: 500, penalty: l2, random_state: 42, solver: lbfgs</td>\n",
       "      <td>0.752297</td>\n",
       "      <td>0.845362</td>\n",
       "      <td>0.765777</td>\n",
       "      <td>0.746725</td>\n",
       "      <td>0.802947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>&lt;class 'sklearn.linear_model.logistic.LogisticRegression'&gt;.- C: 1.0, max_iter: 200, penalty: l2, random_state: 42, solver: lbfgs</td>\n",
       "      <td>0.752297</td>\n",
       "      <td>0.845362</td>\n",
       "      <td>0.765777</td>\n",
       "      <td>0.746725</td>\n",
       "      <td>0.802947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>&lt;class 'sklearn.linear_model.logistic.LogisticRegression'&gt;.- C: 1.0, max_iter: 100, penalty: l2, random_state: 42, solver: lbfgs</td>\n",
       "      <td>0.752297</td>\n",
       "      <td>0.845362</td>\n",
       "      <td>0.765777</td>\n",
       "      <td>0.746725</td>\n",
       "      <td>0.802947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>&lt;class 'sklearn.ensemble.weight_boosting.AdaBoostClassifier'&gt;.- learning_rate: 0.1, n_estimators: 200, random_state: 42</td>\n",
       "      <td>0.748956</td>\n",
       "      <td>0.859209</td>\n",
       "      <td>0.758650</td>\n",
       "      <td>0.745890</td>\n",
       "      <td>0.804201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>&lt;class 'sklearn.ensemble.forest.RandomForestClassifier'&gt;.- criterion: gini, n_estimators: 30, random_state: 42</td>\n",
       "      <td>0.754804</td>\n",
       "      <td>0.817678</td>\n",
       "      <td>0.784182</td>\n",
       "      <td>0.745597</td>\n",
       "      <td>0.799736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>&lt;class 'sklearn.ensemble.weight_boosting.AdaBoostClassifier'&gt;.- learning_rate: 0.1, n_estimators: 150, random_state: 42</td>\n",
       "      <td>0.748120</td>\n",
       "      <td>0.861027</td>\n",
       "      <td>0.757072</td>\n",
       "      <td>0.744977</td>\n",
       "      <td>0.804109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>&lt;class 'sklearn.ensemble.weight_boosting.AdaBoostClassifier'&gt;.- learning_rate: 0.1, n_estimators: 100, random_state: 42</td>\n",
       "      <td>0.746867</td>\n",
       "      <td>0.859070</td>\n",
       "      <td>0.756518</td>\n",
       "      <td>0.743718</td>\n",
       "      <td>0.802895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>&lt;class 'sklearn.ensemble.forest.RandomForestClassifier'&gt;.- criterion: entropy, n_estimators: 30, random_state: 42</td>\n",
       "      <td>0.751880</td>\n",
       "      <td>0.817954</td>\n",
       "      <td>0.779931</td>\n",
       "      <td>0.743178</td>\n",
       "      <td>0.797648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>&lt;class 'sklearn.ensemble.weight_boosting.AdaBoostClassifier'&gt;.- learning_rate: 0.1, n_estimators: 30, random_state: 42</td>\n",
       "      <td>0.732665</td>\n",
       "      <td>0.897715</td>\n",
       "      <td>0.728681</td>\n",
       "      <td>0.741834</td>\n",
       "      <td>0.801752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>&lt;class 'sklearn.ensemble.weight_boosting.AdaBoostClassifier'&gt;.- learning_rate: 0.1, n_estimators: 50, random_state: 42</td>\n",
       "      <td>0.739766</td>\n",
       "      <td>0.877276</td>\n",
       "      <td>0.742460</td>\n",
       "      <td>0.740824</td>\n",
       "      <td>0.802189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>&lt;class 'sklearn.ensemble.weight_boosting.AdaBoostClassifier'&gt;.- learning_rate: 0.5, n_estimators: 30, random_state: 42</td>\n",
       "      <td>0.743943</td>\n",
       "      <td>0.848485</td>\n",
       "      <td>0.756143</td>\n",
       "      <td>0.738148</td>\n",
       "      <td>0.798698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>&lt;class 'sklearn.ensemble.weight_boosting.AdaBoostClassifier'&gt;.- learning_rate: 0.7, n_estimators: 50, random_state: 42</td>\n",
       "      <td>0.743525</td>\n",
       "      <td>0.845569</td>\n",
       "      <td>0.756982</td>\n",
       "      <td>0.737784</td>\n",
       "      <td>0.797836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>&lt;class 'sklearn.ensemble.weight_boosting.AdaBoostClassifier'&gt;.- learning_rate: 0.5, n_estimators: 100, random_state: 42</td>\n",
       "      <td>0.743525</td>\n",
       "      <td>0.833887</td>\n",
       "      <td>0.761298</td>\n",
       "      <td>0.736070</td>\n",
       "      <td>0.795249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>&lt;class 'sklearn.ensemble.weight_boosting.AdaBoostClassifier'&gt;.- learning_rate: 0.5, n_estimators: 150, random_state: 42</td>\n",
       "      <td>0.743108</td>\n",
       "      <td>0.827462</td>\n",
       "      <td>0.763557</td>\n",
       "      <td>0.734483</td>\n",
       "      <td>0.793721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>&lt;class 'sklearn.ensemble.weight_boosting.AdaBoostClassifier'&gt;.- learning_rate: 0.7, n_estimators: 30, random_state: 42</td>\n",
       "      <td>0.737260</td>\n",
       "      <td>0.842100</td>\n",
       "      <td>0.752358</td>\n",
       "      <td>0.731085</td>\n",
       "      <td>0.793409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>&lt;class 'sklearn.ensemble.weight_boosting.AdaBoostClassifier'&gt;.- learning_rate: 0.7, n_estimators: 200, random_state: 42</td>\n",
       "      <td>0.735589</td>\n",
       "      <td>0.757961</td>\n",
       "      <td>0.795390</td>\n",
       "      <td>0.729720</td>\n",
       "      <td>0.772560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>&lt;class 'sklearn.ensemble.weight_boosting.AdaBoostClassifier'&gt;.- learning_rate: 1.0, n_estimators: 30, random_state: 42</td>\n",
       "      <td>0.733500</td>\n",
       "      <td>0.850896</td>\n",
       "      <td>0.747015</td>\n",
       "      <td>0.729288</td>\n",
       "      <td>0.793321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>&lt;class 'sklearn.ensemble.weight_boosting.AdaBoostClassifier'&gt;.- learning_rate: 0.5, n_estimators: 50, random_state: 42</td>\n",
       "      <td>0.736424</td>\n",
       "      <td>0.833887</td>\n",
       "      <td>0.753318</td>\n",
       "      <td>0.728895</td>\n",
       "      <td>0.790831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>&lt;class 'sklearn.ensemble.weight_boosting.AdaBoostClassifier'&gt;.- learning_rate: 0.7, n_estimators: 150, random_state: 42</td>\n",
       "      <td>0.738095</td>\n",
       "      <td>0.816126</td>\n",
       "      <td>0.762758</td>\n",
       "      <td>0.728624</td>\n",
       "      <td>0.788053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>&lt;class 'sklearn.ensemble.weight_boosting.AdaBoostClassifier'&gt;.- learning_rate: 0.7, n_estimators: 100, random_state: 42</td>\n",
       "      <td>0.736842</td>\n",
       "      <td>0.814100</td>\n",
       "      <td>0.762269</td>\n",
       "      <td>0.727142</td>\n",
       "      <td>0.786864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>&lt;class 'sklearn.ensemble.weight_boosting.AdaBoostClassifier'&gt;.- learning_rate: 1.0, n_estimators: 50, random_state: 42</td>\n",
       "      <td>0.728070</td>\n",
       "      <td>0.848564</td>\n",
       "      <td>0.743923</td>\n",
       "      <td>0.726605</td>\n",
       "      <td>0.789462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>&lt;class 'sklearn.ensemble.weight_boosting.AdaBoostClassifier'&gt;.- learning_rate: 0.5, n_estimators: 200, random_state: 42</td>\n",
       "      <td>0.736007</td>\n",
       "      <td>0.756241</td>\n",
       "      <td>0.790586</td>\n",
       "      <td>0.726602</td>\n",
       "      <td>0.772619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>&lt;class 'sklearn.ensemble.weight_boosting.AdaBoostClassifier'&gt;.- learning_rate: 1.0, n_estimators: 100, random_state: 42</td>\n",
       "      <td>0.718881</td>\n",
       "      <td>0.817678</td>\n",
       "      <td>0.743223</td>\n",
       "      <td>0.709244</td>\n",
       "      <td>0.777172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>&lt;class 'sklearn.ensemble.weight_boosting.AdaBoostClassifier'&gt;.- learning_rate: 1.0, n_estimators: 150, random_state: 42</td>\n",
       "      <td>0.700501</td>\n",
       "      <td>0.747603</td>\n",
       "      <td>0.771388</td>\n",
       "      <td>0.705104</td>\n",
       "      <td>0.747345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>&lt;class 'sklearn.ensemble.weight_boosting.AdaBoostClassifier'&gt;.- learning_rate: 1.0, n_estimators: 200, random_state: 42</td>\n",
       "      <td>0.700501</td>\n",
       "      <td>0.764880</td>\n",
       "      <td>0.761508</td>\n",
       "      <td>0.703590</td>\n",
       "      <td>0.752044</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Accuracy  Precision  \\\n",
       "<class 'sklearn.ensemble.forest.RandomForestCla...  0.766082   0.847220   \n",
       "<class 'sklearn.ensemble.forest.RandomForestCla...  0.765664   0.837395   \n",
       "<class 'sklearn.ensemble.forest.RandomForestCla...  0.761487   0.842515   \n",
       "<class 'sklearn.ensemble.forest.RandomForestCla...  0.761905   0.835952   \n",
       "<class 'sklearn.ensemble.forest.RandomForestCla...  0.760234   0.835300   \n",
       "<class 'sklearn.ensemble.forest.RandomForestCla...  0.759398   0.834925   \n",
       "<class 'sklearn.linear_model.logistic.LogisticR...  0.756475   0.852606   \n",
       "<class 'sklearn.linear_model.logistic.LogisticR...  0.756475   0.852606   \n",
       "<class 'sklearn.linear_model.logistic.LogisticR...  0.756475   0.852606   \n",
       "<class 'sklearn.linear_model.logistic.LogisticR...  0.756475   0.852606   \n",
       "<class 'sklearn.ensemble.forest.RandomForestCla...  0.758981   0.827710   \n",
       "<class 'sklearn.ensemble.forest.RandomForestCla...  0.756475   0.836229   \n",
       "<class 'sklearn.linear_model.logistic.LogisticR...  0.752715   0.846666   \n",
       "<class 'sklearn.linear_model.logistic.LogisticR...  0.752715   0.846666   \n",
       "<class 'sklearn.linear_model.logistic.LogisticR...  0.752715   0.846666   \n",
       "<class 'sklearn.linear_model.logistic.LogisticR...  0.752715   0.846666   \n",
       "<class 'sklearn.linear_model.logistic.LogisticR...  0.752297   0.845362   \n",
       "<class 'sklearn.linear_model.logistic.LogisticR...  0.752297   0.845362   \n",
       "<class 'sklearn.linear_model.logistic.LogisticR...  0.752297   0.845362   \n",
       "<class 'sklearn.linear_model.logistic.LogisticR...  0.752297   0.845362   \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.748956   0.859209   \n",
       "<class 'sklearn.ensemble.forest.RandomForestCla...  0.754804   0.817678   \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.748120   0.861027   \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.746867   0.859070   \n",
       "<class 'sklearn.ensemble.forest.RandomForestCla...  0.751880   0.817954   \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.732665   0.897715   \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.739766   0.877276   \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.743943   0.848485   \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.743525   0.845569   \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.743525   0.833887   \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.743108   0.827462   \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.737260   0.842100   \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.735589   0.757961   \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.733500   0.850896   \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.736424   0.833887   \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.738095   0.816126   \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.736842   0.814100   \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.728070   0.848564   \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.736007   0.756241   \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.718881   0.817678   \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.700501   0.747603   \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.700501   0.764880   \n",
       "\n",
       "                                                      Recall       AUC  \\\n",
       "<class 'sklearn.ensemble.forest.RandomForestCla...  0.782407  0.760322   \n",
       "<class 'sklearn.ensemble.forest.RandomForestCla...  0.787547  0.758589   \n",
       "<class 'sklearn.ensemble.forest.RandomForestCla...  0.779462  0.755344   \n",
       "<class 'sklearn.ensemble.forest.RandomForestCla...  0.783256  0.754496   \n",
       "<class 'sklearn.ensemble.forest.RandomForestCla...  0.782067  0.753085   \n",
       "<class 'sklearn.ensemble.forest.RandomForestCla...  0.780882  0.752806   \n",
       "<class 'sklearn.linear_model.logistic.LogisticR...  0.766587  0.751663   \n",
       "<class 'sklearn.linear_model.logistic.LogisticR...  0.766587  0.751663   \n",
       "<class 'sklearn.linear_model.logistic.LogisticR...  0.766587  0.751663   \n",
       "<class 'sklearn.linear_model.logistic.LogisticR...  0.766587  0.751663   \n",
       "<class 'sklearn.ensemble.forest.RandomForestCla...  0.784440  0.750784   \n",
       "<class 'sklearn.ensemble.forest.RandomForestCla...  0.776360  0.749841   \n",
       "<class 'sklearn.linear_model.logistic.LogisticR...  0.765543  0.747278   \n",
       "<class 'sklearn.linear_model.logistic.LogisticR...  0.765543  0.747278   \n",
       "<class 'sklearn.linear_model.logistic.LogisticR...  0.765543  0.747278   \n",
       "<class 'sklearn.linear_model.logistic.LogisticR...  0.765543  0.747278   \n",
       "<class 'sklearn.linear_model.logistic.LogisticR...  0.765777  0.746725   \n",
       "<class 'sklearn.linear_model.logistic.LogisticR...  0.765777  0.746725   \n",
       "<class 'sklearn.linear_model.logistic.LogisticR...  0.765777  0.746725   \n",
       "<class 'sklearn.linear_model.logistic.LogisticR...  0.765777  0.746725   \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.758650  0.745890   \n",
       "<class 'sklearn.ensemble.forest.RandomForestCla...  0.784182  0.745597   \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.757072  0.744977   \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.756518  0.743718   \n",
       "<class 'sklearn.ensemble.forest.RandomForestCla...  0.779931  0.743178   \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.728681  0.741834   \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.742460  0.740824   \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.756143  0.738148   \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.756982  0.737784   \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.761298  0.736070   \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.763557  0.734483   \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.752358  0.731085   \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.795390  0.729720   \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.747015  0.729288   \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.753318  0.728895   \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.762758  0.728624   \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.762269  0.727142   \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.743923  0.726605   \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.790586  0.726602   \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.743223  0.709244   \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.771388  0.705104   \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.761508  0.703590   \n",
       "\n",
       "                                                          F1  \n",
       "<class 'sklearn.ensemble.forest.RandomForestCla...  0.812546  \n",
       "<class 'sklearn.ensemble.forest.RandomForestCla...  0.810606  \n",
       "<class 'sklearn.ensemble.forest.RandomForestCla...  0.808717  \n",
       "<class 'sklearn.ensemble.forest.RandomForestCla...  0.807827  \n",
       "<class 'sklearn.ensemble.forest.RandomForestCla...  0.806658  \n",
       "<class 'sklearn.ensemble.forest.RandomForestCla...  0.805843  \n",
       "<class 'sklearn.linear_model.logistic.LogisticR...  0.806864  \n",
       "<class 'sklearn.linear_model.logistic.LogisticR...  0.806864  \n",
       "<class 'sklearn.linear_model.logistic.LogisticR...  0.806864  \n",
       "<class 'sklearn.linear_model.logistic.LogisticR...  0.806864  \n",
       "<class 'sklearn.ensemble.forest.RandomForestCla...  0.804483  \n",
       "<class 'sklearn.ensemble.forest.RandomForestCla...  0.804170  \n",
       "<class 'sklearn.linear_model.logistic.LogisticR...  0.803440  \n",
       "<class 'sklearn.linear_model.logistic.LogisticR...  0.803440  \n",
       "<class 'sklearn.linear_model.logistic.LogisticR...  0.803440  \n",
       "<class 'sklearn.linear_model.logistic.LogisticR...  0.803440  \n",
       "<class 'sklearn.linear_model.logistic.LogisticR...  0.802947  \n",
       "<class 'sklearn.linear_model.logistic.LogisticR...  0.802947  \n",
       "<class 'sklearn.linear_model.logistic.LogisticR...  0.802947  \n",
       "<class 'sklearn.linear_model.logistic.LogisticR...  0.802947  \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.804201  \n",
       "<class 'sklearn.ensemble.forest.RandomForestCla...  0.799736  \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.804109  \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.802895  \n",
       "<class 'sklearn.ensemble.forest.RandomForestCla...  0.797648  \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.801752  \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.802189  \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.798698  \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.797836  \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.795249  \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.793721  \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.793409  \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.772560  \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.793321  \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.790831  \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.788053  \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.786864  \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.789462  \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.772619  \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.777172  \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.747345  \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.752044  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# average results considering the 3 different splits \n",
    "cl.average_df([split1_results, split2_results, split3_results], 'AUC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'criterion': 'entropy', 'n_estimators': 200, 'random_state': 42}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_grid[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The best model params:\n",
    "best_model_params = rf_grid[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the best model with the 3 different splits\n",
    "mod_split1 = cl.specific_model(RandomForestClassifier, splits['split_1'], best_model_params)\n",
    "mod_split2 = cl.specific_model(RandomForestClassifier, splits['split_2'], best_model_params)\n",
    "mod_split3 = cl.specific_model(RandomForestClassifier, splits['split_3'], best_model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the most important features in the 3 versions of the best model\n",
    "imp_1 = cl.important_features(mod_split1)\n",
    "imp_2 = cl.important_features(mod_split2)\n",
    "imp_3 = cl.important_features(mod_split3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.084644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.072916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.072435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.054846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.052130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.050884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.045986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.044393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.043865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.039037</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         imp\n",
       "10  0.084644\n",
       "17  0.072916\n",
       "6   0.072435\n",
       "26  0.054846\n",
       "20  0.052130\n",
       "16  0.050884\n",
       "11  0.045986\n",
       "25  0.044393\n",
       "2   0.043865\n",
       "3   0.039037"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Average most important features \n",
    "cl.average_df([imp_1, imp_2, imp_3], 'imp')[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = data_2014.columns[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "garbage\n",
      "S1401_C02_001E\n",
      "weapons_violation\n",
      "S2301_C03_001E\n",
      "S1501_C02_008E\n",
      "S1101_C01_002E\n",
      "sanitation\n",
      "S2201_C02_001E\n",
      "vehicle_theft\n",
      "burglary\n"
     ]
    }
   ],
   "source": [
    "for i in [10, 17, 6, 26, 20, 16, 11, 25, 2, 3]:\n",
    "    print(features[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This are the most important features descriptions\n",
    "'''\n",
    "'garbage'\n",
    "'Population 3 years and over enrolled in school'\n",
    "'weapons_violation'\n",
    "'Employment/Population Ratio!!Estimate!!Population 16 years and over'\n",
    "'Population 25 years and over!!9th to 12th grade, no diploma'\n",
    "'Average household size'\n",
    "'sanitation'\n",
    "'Households. FOOD STAMPS/Supplemental Nutrition Assistance Program (SNAP)'\n",
    "'vehicle_theft'\n",
    "'burglary'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Predictions for 2019 considering data from 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_2017 = pd.read_csv('clean_data/complete_2017.csv')\n",
    "tracts = inputs_2017['tract'].reset_index()\n",
    "inputs_2017.drop(['tract', 'S0101_C01_001E'], axis=1, inplace=True)\n",
    "inputs_2017 = cl.standardrize(inputs_2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "proba_1 = pd.DataFrame(mod_split1.predict_proba(inputs_2017))[[1]]\n",
    "proba_2 = pd.DataFrame(mod_split2.predict_proba(inputs_2017))[[1]]\n",
    "proba_3 = pd.DataFrame(mod_split3.predict_proba(inputs_2017))[[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average predict proba \n",
    "proba_avg = cl.average_df([proba_1, proba_2, proba_3], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>probability</th>\n",
       "      <th>tract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.996667</td>\n",
       "      <td>290900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.996667</td>\n",
       "      <td>671500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.996667</td>\n",
       "      <td>250800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.995000</td>\n",
       "      <td>691500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.995000</td>\n",
       "      <td>711000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   probability   tract\n",
       "0     0.996667  290900\n",
       "1     0.996667  671500\n",
       "2     0.996667  250800\n",
       "3     0.995000  691500\n",
       "4     0.995000  711000"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Average predict proba by tract\n",
    "probs_2019 = proba_avg.reset_index()\n",
    "probs_2019 = probs_2019.merge(tracts, on='index')\n",
    "probs_2019.drop('index', axis=1, inplace = True)\n",
    "probs_2019.columns = ['probability', 'tract']\n",
    "probs_2019.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a Geopandas data frame to create a map\n",
    "tracts_geo = gpd.read_file('raw_data\\geo_export_fe9f2155-ba22-4697-91ff-daeee48c8d0b.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracts_geo = tracts_geo[['tractce10', 'geometry']]\n",
    "tracts_geo['tractce10'] = tracts_geo['tractce10'].astype(int)\n",
    "abandon_2019_pred = tracts_geo.merge(probs_2019, left_on='tractce10', right_on='tract', how='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "abandon_2019_pred.to_file('clean_data/abandon_2019_pred.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
