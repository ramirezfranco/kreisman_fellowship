{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import classifiers as cl\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "import geopandas as gpd\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2014 = pd.read_csv('clean_data/data_2014.csv')\n",
    "data_2015 = pd.read_csv('clean_data/data_2015.csv')\n",
    "data_2016 = pd.read_csv('clean_data/data_2016.csv')\n",
    "data_2017 = pd.read_csv('clean_data/data_2017.csv')\n",
    "\n",
    "data_2014 = data_2014.fillna(0).drop('S0101_C01_001E', axis=1)\n",
    "data_2015 = data_2015.fillna(0).drop('S0101_C01_001E', axis=1)\n",
    "data_2016 = data_2016.fillna(0).drop('S0101_C01_001E', axis=1)\n",
    "data_2017 = data_2017.fillna(0).drop('S0101_C01_001E', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits={\n",
    "    'split_1':{\n",
    "        'x_train':cl.standardrize(data_2014.iloc[:,1:]),\n",
    "        'x_test':cl.standardrize(data_2015.iloc[:,1:]),\n",
    "        'y_train':data_2014['risk'],\n",
    "        'y_test':data_2015['risk']\n",
    "    },\n",
    "    'split_2':{\n",
    "        'x_train':cl.standardrize(data_2014.iloc[:,1:].append(data_2015.iloc[:,1:])),\n",
    "        'x_test':cl.standardrize(data_2016.iloc[:,1:]),\n",
    "        'y_train':data_2014['risk'].append(data_2015['risk']),\n",
    "        'y_test':data_2016['risk']\n",
    "    },\n",
    "        'split_3':{\n",
    "        'x_train':cl.standardrize(data_2014.iloc[:,1:].append(data_2015.iloc[:,1:]).append(data_2016.iloc[:,1:])),\n",
    "        'x_test':cl.standardrize(data_2017.iloc[:,1:]),\n",
    "        'y_train':data_2014['risk'].append(data_2015['risk']).append(data_2016['risk']),\n",
    "        'y_test':data_2017['risk']\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating param grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_params = {\n",
    "    'penalty':['l2'],\n",
    "    'C':[0.1, 0.5, 1.0],\n",
    "    'random_state':[42],\n",
    "    'max_iter':[100, 200, 300, 500],\n",
    "    'solver': ['lbfgs']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_params = {\n",
    "    'C':[0.1, 0.5, 1.0],\n",
    "    'kernel':['linear', 'poly', 'rbf'],\n",
    "    'gamma':['auto', 'scale'],\n",
    "    'max_iter':[100, 200, 300, 500, 1000],\n",
    "    'random_state':[42]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "boost_params ={\n",
    "    'n_estimators':[30, 50, 100, 150, 200],\n",
    "    'learning_rate':[0.1, 0.5, 0.7, 1.0],\n",
    "    'random_state':[42]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_params = {\n",
    "    'n_estimators':[30, 50, 100, 150, 200],\n",
    "    'criterion':['gini', 'entropy'],\n",
    "    'random_state':[42]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_grid = list(ParameterGrid(logistic_params))\n",
    "svm_grid = list(ParameterGrid(svm_params))\n",
    "boost_grid = list(ParameterGrid(boost_params))\n",
    "rf_grid = list(ParameterGrid(rf_params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Testing different models for different splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_models = {\n",
    "    'logistic':{'model':LogisticRegression, 'grid':logistic_grid},\n",
    "    'boost':{'model':AdaBoostClassifier, 'grid':boost_grid},\n",
    "    'rf':{'model':RandomForestClassifier, 'grid':rf_grid}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "split1_results = cl.different_models(diff_models, splits['split_1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "split2_results = cl.different_models(diff_models, splits['split_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "split3_results = cl.different_models(diff_models, splits['split_3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>AUC</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>&lt;class 'sklearn.ensemble.forest.RandomForestClassifier'&gt;.- criterion: entropy, n_estimators: 200, random_state: 42</td>\n",
       "      <td>0.780702</td>\n",
       "      <td>0.875469</td>\n",
       "      <td>0.777385</td>\n",
       "      <td>0.784653</td>\n",
       "      <td>0.820988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>&lt;class 'sklearn.ensemble.forest.RandomForestClassifier'&gt;.- criterion: gini, n_estimators: 200, random_state: 42</td>\n",
       "      <td>0.780284</td>\n",
       "      <td>0.865107</td>\n",
       "      <td>0.782496</td>\n",
       "      <td>0.782607</td>\n",
       "      <td>0.818897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>&lt;class 'sklearn.ensemble.forest.RandomForestClassifier'&gt;.- criterion: entropy, n_estimators: 150, random_state: 42</td>\n",
       "      <td>0.778613</td>\n",
       "      <td>0.872750</td>\n",
       "      <td>0.776311</td>\n",
       "      <td>0.782269</td>\n",
       "      <td>0.819150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>&lt;class 'sklearn.linear_model.logistic.LogisticRegression'&gt;.- C: 0.1, max_iter: 200, penalty: l2, random_state: 42, solver: lbfgs</td>\n",
       "      <td>0.776942</td>\n",
       "      <td>0.885279</td>\n",
       "      <td>0.765949</td>\n",
       "      <td>0.781647</td>\n",
       "      <td>0.819972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>&lt;class 'sklearn.linear_model.logistic.LogisticRegression'&gt;.- C: 0.1, max_iter: 100, penalty: l2, random_state: 42, solver: lbfgs</td>\n",
       "      <td>0.776942</td>\n",
       "      <td>0.885279</td>\n",
       "      <td>0.765949</td>\n",
       "      <td>0.781647</td>\n",
       "      <td>0.819972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>&lt;class 'sklearn.linear_model.logistic.LogisticRegression'&gt;.- C: 0.1, max_iter: 500, penalty: l2, random_state: 42, solver: lbfgs</td>\n",
       "      <td>0.776942</td>\n",
       "      <td>0.885279</td>\n",
       "      <td>0.765949</td>\n",
       "      <td>0.781647</td>\n",
       "      <td>0.819972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>&lt;class 'sklearn.linear_model.logistic.LogisticRegression'&gt;.- C: 0.1, max_iter: 300, penalty: l2, random_state: 42, solver: lbfgs</td>\n",
       "      <td>0.776942</td>\n",
       "      <td>0.885279</td>\n",
       "      <td>0.765949</td>\n",
       "      <td>0.781647</td>\n",
       "      <td>0.819972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>&lt;class 'sklearn.ensemble.forest.RandomForestClassifier'&gt;.- criterion: entropy, n_estimators: 50, random_state: 42</td>\n",
       "      <td>0.778195</td>\n",
       "      <td>0.865975</td>\n",
       "      <td>0.778952</td>\n",
       "      <td>0.780412</td>\n",
       "      <td>0.817658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>&lt;class 'sklearn.ensemble.forest.RandomForestClassifier'&gt;.- criterion: gini, n_estimators: 100, random_state: 42</td>\n",
       "      <td>0.775689</td>\n",
       "      <td>0.862853</td>\n",
       "      <td>0.777563</td>\n",
       "      <td>0.777480</td>\n",
       "      <td>0.815431</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>&lt;class 'sklearn.ensemble.forest.RandomForestClassifier'&gt;.- criterion: gini, n_estimators: 150, random_state: 42</td>\n",
       "      <td>0.774854</td>\n",
       "      <td>0.863102</td>\n",
       "      <td>0.777045</td>\n",
       "      <td>0.777416</td>\n",
       "      <td>0.814918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>&lt;class 'sklearn.ensemble.forest.RandomForestClassifier'&gt;.- criterion: gini, n_estimators: 50, random_state: 42</td>\n",
       "      <td>0.776107</td>\n",
       "      <td>0.857586</td>\n",
       "      <td>0.781272</td>\n",
       "      <td>0.777386</td>\n",
       "      <td>0.814845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>&lt;class 'sklearn.ensemble.weight_boosting.AdaBoostClassifier'&gt;.- learning_rate: 0.1, n_estimators: 150, random_state: 42</td>\n",
       "      <td>0.763576</td>\n",
       "      <td>0.890714</td>\n",
       "      <td>0.753070</td>\n",
       "      <td>0.777084</td>\n",
       "      <td>0.812343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>&lt;class 'sklearn.linear_model.logistic.LogisticRegression'&gt;.- C: 0.5, max_iter: 100, penalty: l2, random_state: 42, solver: lbfgs</td>\n",
       "      <td>0.773183</td>\n",
       "      <td>0.879160</td>\n",
       "      <td>0.764903</td>\n",
       "      <td>0.777043</td>\n",
       "      <td>0.816531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>&lt;class 'sklearn.linear_model.logistic.LogisticRegression'&gt;.- C: 0.5, max_iter: 300, penalty: l2, random_state: 42, solver: lbfgs</td>\n",
       "      <td>0.773183</td>\n",
       "      <td>0.879160</td>\n",
       "      <td>0.764903</td>\n",
       "      <td>0.777043</td>\n",
       "      <td>0.816531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>&lt;class 'sklearn.linear_model.logistic.LogisticRegression'&gt;.- C: 0.5, max_iter: 500, penalty: l2, random_state: 42, solver: lbfgs</td>\n",
       "      <td>0.773183</td>\n",
       "      <td>0.879160</td>\n",
       "      <td>0.764903</td>\n",
       "      <td>0.777043</td>\n",
       "      <td>0.816531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>&lt;class 'sklearn.linear_model.logistic.LogisticRegression'&gt;.- C: 0.5, max_iter: 200, penalty: l2, random_state: 42, solver: lbfgs</td>\n",
       "      <td>0.773183</td>\n",
       "      <td>0.879160</td>\n",
       "      <td>0.764903</td>\n",
       "      <td>0.777043</td>\n",
       "      <td>0.816531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>&lt;class 'sklearn.ensemble.forest.RandomForestClassifier'&gt;.- criterion: entropy, n_estimators: 100, random_state: 42</td>\n",
       "      <td>0.774436</td>\n",
       "      <td>0.866648</td>\n",
       "      <td>0.773791</td>\n",
       "      <td>0.776633</td>\n",
       "      <td>0.815315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>&lt;class 'sklearn.linear_model.logistic.LogisticRegression'&gt;.- C: 1.0, max_iter: 300, penalty: l2, random_state: 42, solver: lbfgs</td>\n",
       "      <td>0.772765</td>\n",
       "      <td>0.877856</td>\n",
       "      <td>0.765136</td>\n",
       "      <td>0.776380</td>\n",
       "      <td>0.816065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>&lt;class 'sklearn.linear_model.logistic.LogisticRegression'&gt;.- C: 1.0, max_iter: 100, penalty: l2, random_state: 42, solver: lbfgs</td>\n",
       "      <td>0.772765</td>\n",
       "      <td>0.877856</td>\n",
       "      <td>0.765136</td>\n",
       "      <td>0.776380</td>\n",
       "      <td>0.816065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>&lt;class 'sklearn.linear_model.logistic.LogisticRegression'&gt;.- C: 1.0, max_iter: 200, penalty: l2, random_state: 42, solver: lbfgs</td>\n",
       "      <td>0.772765</td>\n",
       "      <td>0.877856</td>\n",
       "      <td>0.765136</td>\n",
       "      <td>0.776380</td>\n",
       "      <td>0.816065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>&lt;class 'sklearn.linear_model.logistic.LogisticRegression'&gt;.- C: 1.0, max_iter: 500, penalty: l2, random_state: 42, solver: lbfgs</td>\n",
       "      <td>0.772765</td>\n",
       "      <td>0.877856</td>\n",
       "      <td>0.765136</td>\n",
       "      <td>0.776380</td>\n",
       "      <td>0.816065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>&lt;class 'sklearn.ensemble.weight_boosting.AdaBoostClassifier'&gt;.- learning_rate: 0.1, n_estimators: 200, random_state: 42</td>\n",
       "      <td>0.762740</td>\n",
       "      <td>0.887273</td>\n",
       "      <td>0.753450</td>\n",
       "      <td>0.774964</td>\n",
       "      <td>0.811211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>&lt;class 'sklearn.ensemble.weight_boosting.AdaBoostClassifier'&gt;.- learning_rate: 0.1, n_estimators: 100, random_state: 42</td>\n",
       "      <td>0.761487</td>\n",
       "      <td>0.887946</td>\n",
       "      <td>0.751920</td>\n",
       "      <td>0.774294</td>\n",
       "      <td>0.810519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>&lt;class 'sklearn.ensemble.weight_boosting.AdaBoostClassifier'&gt;.- learning_rate: 0.1, n_estimators: 50, random_state: 42</td>\n",
       "      <td>0.751880</td>\n",
       "      <td>0.904524</td>\n",
       "      <td>0.736498</td>\n",
       "      <td>0.774273</td>\n",
       "      <td>0.807473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>&lt;class 'sklearn.ensemble.forest.RandomForestClassifier'&gt;.- criterion: gini, n_estimators: 30, random_state: 42</td>\n",
       "      <td>0.772765</td>\n",
       "      <td>0.847739</td>\n",
       "      <td>0.781598</td>\n",
       "      <td>0.772082</td>\n",
       "      <td>0.810810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>&lt;class 'sklearn.ensemble.forest.RandomForestClassifier'&gt;.- criterion: entropy, n_estimators: 30, random_state: 42</td>\n",
       "      <td>0.771512</td>\n",
       "      <td>0.849190</td>\n",
       "      <td>0.778606</td>\n",
       "      <td>0.770199</td>\n",
       "      <td>0.810337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>&lt;class 'sklearn.ensemble.weight_boosting.AdaBoostClassifier'&gt;.- learning_rate: 0.1, n_estimators: 30, random_state: 42</td>\n",
       "      <td>0.736424</td>\n",
       "      <td>0.918107</td>\n",
       "      <td>0.717895</td>\n",
       "      <td>0.769998</td>\n",
       "      <td>0.800613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>&lt;class 'sklearn.ensemble.weight_boosting.AdaBoostClassifier'&gt;.- learning_rate: 0.5, n_estimators: 100, random_state: 42</td>\n",
       "      <td>0.763158</td>\n",
       "      <td>0.865212</td>\n",
       "      <td>0.760021</td>\n",
       "      <td>0.765021</td>\n",
       "      <td>0.807449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>&lt;class 'sklearn.ensemble.weight_boosting.AdaBoostClassifier'&gt;.- learning_rate: 0.7, n_estimators: 50, random_state: 42</td>\n",
       "      <td>0.756475</td>\n",
       "      <td>0.872286</td>\n",
       "      <td>0.750943</td>\n",
       "      <td>0.762543</td>\n",
       "      <td>0.804457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>&lt;class 'sklearn.ensemble.weight_boosting.AdaBoostClassifier'&gt;.- learning_rate: 0.5, n_estimators: 30, random_state: 42</td>\n",
       "      <td>0.755221</td>\n",
       "      <td>0.872774</td>\n",
       "      <td>0.748936</td>\n",
       "      <td>0.761293</td>\n",
       "      <td>0.803568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>&lt;class 'sklearn.ensemble.weight_boosting.AdaBoostClassifier'&gt;.- learning_rate: 0.5, n_estimators: 150, random_state: 42</td>\n",
       "      <td>0.761069</td>\n",
       "      <td>0.856986</td>\n",
       "      <td>0.760983</td>\n",
       "      <td>0.761173</td>\n",
       "      <td>0.804519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>&lt;class 'sklearn.ensemble.weight_boosting.AdaBoostClassifier'&gt;.- learning_rate: 0.5, n_estimators: 50, random_state: 42</td>\n",
       "      <td>0.754386</td>\n",
       "      <td>0.863410</td>\n",
       "      <td>0.750807</td>\n",
       "      <td>0.756978</td>\n",
       "      <td>0.801304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>&lt;class 'sklearn.ensemble.weight_boosting.AdaBoostClassifier'&gt;.- learning_rate: 0.7, n_estimators: 200, random_state: 42</td>\n",
       "      <td>0.754386</td>\n",
       "      <td>0.788027</td>\n",
       "      <td>0.793425</td>\n",
       "      <td>0.756419</td>\n",
       "      <td>0.784202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>&lt;class 'sklearn.ensemble.weight_boosting.AdaBoostClassifier'&gt;.- learning_rate: 0.7, n_estimators: 150, random_state: 42</td>\n",
       "      <td>0.758563</td>\n",
       "      <td>0.847366</td>\n",
       "      <td>0.762093</td>\n",
       "      <td>0.756350</td>\n",
       "      <td>0.801181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>&lt;class 'sklearn.ensemble.weight_boosting.AdaBoostClassifier'&gt;.- learning_rate: 0.7, n_estimators: 30, random_state: 42</td>\n",
       "      <td>0.748538</td>\n",
       "      <td>0.867284</td>\n",
       "      <td>0.745278</td>\n",
       "      <td>0.755443</td>\n",
       "      <td>0.798441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>&lt;class 'sklearn.ensemble.weight_boosting.AdaBoostClassifier'&gt;.- learning_rate: 0.7, n_estimators: 100, random_state: 42</td>\n",
       "      <td>0.757310</td>\n",
       "      <td>0.845161</td>\n",
       "      <td>0.761605</td>\n",
       "      <td>0.754963</td>\n",
       "      <td>0.799886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>&lt;class 'sklearn.ensemble.weight_boosting.AdaBoostClassifier'&gt;.- learning_rate: 1.0, n_estimators: 30, random_state: 42</td>\n",
       "      <td>0.741437</td>\n",
       "      <td>0.873821</td>\n",
       "      <td>0.738170</td>\n",
       "      <td>0.754780</td>\n",
       "      <td>0.795427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>&lt;class 'sklearn.ensemble.weight_boosting.AdaBoostClassifier'&gt;.- learning_rate: 0.5, n_estimators: 200, random_state: 42</td>\n",
       "      <td>0.760652</td>\n",
       "      <td>0.788672</td>\n",
       "      <td>0.793562</td>\n",
       "      <td>0.754281</td>\n",
       "      <td>0.789903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>&lt;class 'sklearn.ensemble.weight_boosting.AdaBoostClassifier'&gt;.- learning_rate: 1.0, n_estimators: 50, random_state: 42</td>\n",
       "      <td>0.727652</td>\n",
       "      <td>0.864632</td>\n",
       "      <td>0.730100</td>\n",
       "      <td>0.744061</td>\n",
       "      <td>0.785246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>&lt;class 'sklearn.ensemble.weight_boosting.AdaBoostClassifier'&gt;.- learning_rate: 1.0, n_estimators: 100, random_state: 42</td>\n",
       "      <td>0.729323</td>\n",
       "      <td>0.840439</td>\n",
       "      <td>0.735401</td>\n",
       "      <td>0.731289</td>\n",
       "      <td>0.781091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>&lt;class 'sklearn.ensemble.weight_boosting.AdaBoostClassifier'&gt;.- learning_rate: 1.0, n_estimators: 150, random_state: 42</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.773340</td>\n",
       "      <td>0.765801</td>\n",
       "      <td>0.729131</td>\n",
       "      <td>0.754263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>&lt;class 'sklearn.ensemble.weight_boosting.AdaBoostClassifier'&gt;.- learning_rate: 1.0, n_estimators: 200, random_state: 42</td>\n",
       "      <td>0.711779</td>\n",
       "      <td>0.788810</td>\n",
       "      <td>0.754327</td>\n",
       "      <td>0.726970</td>\n",
       "      <td>0.756740</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Accuracy  Precision  \\\n",
       "<class 'sklearn.ensemble.forest.RandomForestCla...  0.780702   0.875469   \n",
       "<class 'sklearn.ensemble.forest.RandomForestCla...  0.780284   0.865107   \n",
       "<class 'sklearn.ensemble.forest.RandomForestCla...  0.778613   0.872750   \n",
       "<class 'sklearn.linear_model.logistic.LogisticR...  0.776942   0.885279   \n",
       "<class 'sklearn.linear_model.logistic.LogisticR...  0.776942   0.885279   \n",
       "<class 'sklearn.linear_model.logistic.LogisticR...  0.776942   0.885279   \n",
       "<class 'sklearn.linear_model.logistic.LogisticR...  0.776942   0.885279   \n",
       "<class 'sklearn.ensemble.forest.RandomForestCla...  0.778195   0.865975   \n",
       "<class 'sklearn.ensemble.forest.RandomForestCla...  0.775689   0.862853   \n",
       "<class 'sklearn.ensemble.forest.RandomForestCla...  0.774854   0.863102   \n",
       "<class 'sklearn.ensemble.forest.RandomForestCla...  0.776107   0.857586   \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.763576   0.890714   \n",
       "<class 'sklearn.linear_model.logistic.LogisticR...  0.773183   0.879160   \n",
       "<class 'sklearn.linear_model.logistic.LogisticR...  0.773183   0.879160   \n",
       "<class 'sklearn.linear_model.logistic.LogisticR...  0.773183   0.879160   \n",
       "<class 'sklearn.linear_model.logistic.LogisticR...  0.773183   0.879160   \n",
       "<class 'sklearn.ensemble.forest.RandomForestCla...  0.774436   0.866648   \n",
       "<class 'sklearn.linear_model.logistic.LogisticR...  0.772765   0.877856   \n",
       "<class 'sklearn.linear_model.logistic.LogisticR...  0.772765   0.877856   \n",
       "<class 'sklearn.linear_model.logistic.LogisticR...  0.772765   0.877856   \n",
       "<class 'sklearn.linear_model.logistic.LogisticR...  0.772765   0.877856   \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.762740   0.887273   \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.761487   0.887946   \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.751880   0.904524   \n",
       "<class 'sklearn.ensemble.forest.RandomForestCla...  0.772765   0.847739   \n",
       "<class 'sklearn.ensemble.forest.RandomForestCla...  0.771512   0.849190   \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.736424   0.918107   \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.763158   0.865212   \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.756475   0.872286   \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.755221   0.872774   \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.761069   0.856986   \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.754386   0.863410   \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.754386   0.788027   \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.758563   0.847366   \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.748538   0.867284   \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.757310   0.845161   \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.741437   0.873821   \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.760652   0.788672   \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.727652   0.864632   \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.729323   0.840439   \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.714286   0.773340   \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.711779   0.788810   \n",
       "\n",
       "                                                      Recall       AUC  \\\n",
       "<class 'sklearn.ensemble.forest.RandomForestCla...  0.777385  0.784653   \n",
       "<class 'sklearn.ensemble.forest.RandomForestCla...  0.782496  0.782607   \n",
       "<class 'sklearn.ensemble.forest.RandomForestCla...  0.776311  0.782269   \n",
       "<class 'sklearn.linear_model.logistic.LogisticR...  0.765949  0.781647   \n",
       "<class 'sklearn.linear_model.logistic.LogisticR...  0.765949  0.781647   \n",
       "<class 'sklearn.linear_model.logistic.LogisticR...  0.765949  0.781647   \n",
       "<class 'sklearn.linear_model.logistic.LogisticR...  0.765949  0.781647   \n",
       "<class 'sklearn.ensemble.forest.RandomForestCla...  0.778952  0.780412   \n",
       "<class 'sklearn.ensemble.forest.RandomForestCla...  0.777563  0.777480   \n",
       "<class 'sklearn.ensemble.forest.RandomForestCla...  0.777045  0.777416   \n",
       "<class 'sklearn.ensemble.forest.RandomForestCla...  0.781272  0.777386   \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.753070  0.777084   \n",
       "<class 'sklearn.linear_model.logistic.LogisticR...  0.764903  0.777043   \n",
       "<class 'sklearn.linear_model.logistic.LogisticR...  0.764903  0.777043   \n",
       "<class 'sklearn.linear_model.logistic.LogisticR...  0.764903  0.777043   \n",
       "<class 'sklearn.linear_model.logistic.LogisticR...  0.764903  0.777043   \n",
       "<class 'sklearn.ensemble.forest.RandomForestCla...  0.773791  0.776633   \n",
       "<class 'sklearn.linear_model.logistic.LogisticR...  0.765136  0.776380   \n",
       "<class 'sklearn.linear_model.logistic.LogisticR...  0.765136  0.776380   \n",
       "<class 'sklearn.linear_model.logistic.LogisticR...  0.765136  0.776380   \n",
       "<class 'sklearn.linear_model.logistic.LogisticR...  0.765136  0.776380   \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.753450  0.774964   \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.751920  0.774294   \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.736498  0.774273   \n",
       "<class 'sklearn.ensemble.forest.RandomForestCla...  0.781598  0.772082   \n",
       "<class 'sklearn.ensemble.forest.RandomForestCla...  0.778606  0.770199   \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.717895  0.769998   \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.760021  0.765021   \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.750943  0.762543   \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.748936  0.761293   \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.760983  0.761173   \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.750807  0.756978   \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.793425  0.756419   \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.762093  0.756350   \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.745278  0.755443   \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.761605  0.754963   \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.738170  0.754780   \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.793562  0.754281   \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.730100  0.744061   \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.735401  0.731289   \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.765801  0.729131   \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.754327  0.726970   \n",
       "\n",
       "                                                          F1  \n",
       "<class 'sklearn.ensemble.forest.RandomForestCla...  0.820988  \n",
       "<class 'sklearn.ensemble.forest.RandomForestCla...  0.818897  \n",
       "<class 'sklearn.ensemble.forest.RandomForestCla...  0.819150  \n",
       "<class 'sklearn.linear_model.logistic.LogisticR...  0.819972  \n",
       "<class 'sklearn.linear_model.logistic.LogisticR...  0.819972  \n",
       "<class 'sklearn.linear_model.logistic.LogisticR...  0.819972  \n",
       "<class 'sklearn.linear_model.logistic.LogisticR...  0.819972  \n",
       "<class 'sklearn.ensemble.forest.RandomForestCla...  0.817658  \n",
       "<class 'sklearn.ensemble.forest.RandomForestCla...  0.815431  \n",
       "<class 'sklearn.ensemble.forest.RandomForestCla...  0.814918  \n",
       "<class 'sklearn.ensemble.forest.RandomForestCla...  0.814845  \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.812343  \n",
       "<class 'sklearn.linear_model.logistic.LogisticR...  0.816531  \n",
       "<class 'sklearn.linear_model.logistic.LogisticR...  0.816531  \n",
       "<class 'sklearn.linear_model.logistic.LogisticR...  0.816531  \n",
       "<class 'sklearn.linear_model.logistic.LogisticR...  0.816531  \n",
       "<class 'sklearn.ensemble.forest.RandomForestCla...  0.815315  \n",
       "<class 'sklearn.linear_model.logistic.LogisticR...  0.816065  \n",
       "<class 'sklearn.linear_model.logistic.LogisticR...  0.816065  \n",
       "<class 'sklearn.linear_model.logistic.LogisticR...  0.816065  \n",
       "<class 'sklearn.linear_model.logistic.LogisticR...  0.816065  \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.811211  \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.810519  \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.807473  \n",
       "<class 'sklearn.ensemble.forest.RandomForestCla...  0.810810  \n",
       "<class 'sklearn.ensemble.forest.RandomForestCla...  0.810337  \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.800613  \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.807449  \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.804457  \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.803568  \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.804519  \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.801304  \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.784202  \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.801181  \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.798441  \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.799886  \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.795427  \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.789903  \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.785246  \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.781091  \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.754263  \n",
       "<class 'sklearn.ensemble.weight_boosting.AdaBoo...  0.756740  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# average results considering the 3 different splits \n",
    "cl.average_df([split1_results, split2_results, split3_results], 'AUC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'criterion': 'entropy', 'n_estimators': 200, 'random_state': 42}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_grid[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The best model params:\n",
    "best_model_params = rf_grid[9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the best model with the 3 different splits\n",
    "mod_split1 = cl.specific_model(RandomForestClassifier, splits['split_1'], best_model_params)\n",
    "mod_split2 = cl.specific_model(RandomForestClassifier, splits['split_2'], best_model_params)\n",
    "mod_split3 = cl.specific_model(RandomForestClassifier, splits['split_3'], best_model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the most important features in the 3 versions of the best model\n",
    "imp_1 = cl.important_features(mod_split1)\n",
    "imp_2 = cl.important_features(mod_split2)\n",
    "imp_3 = cl.important_features(mod_split3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.084644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.072916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.072435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.054846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.052130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.050884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.045986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.044393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.043865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.039037</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         imp\n",
       "10  0.084644\n",
       "17  0.072916\n",
       "6   0.072435\n",
       "26  0.054846\n",
       "20  0.052130\n",
       "16  0.050884\n",
       "11  0.045986\n",
       "25  0.044393\n",
       "2   0.043865\n",
       "3   0.039037"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Average most important features \n",
    "cl.average_df([imp_1, imp_2, imp_3], 'imp')[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = data_2014.columns[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "garbage\n",
      "S1401_C02_001E\n",
      "weapons_violation\n",
      "S2301_C03_001E\n",
      "S1501_C02_008E\n",
      "S1101_C01_002E\n",
      "sanitation\n",
      "S2201_C02_001E\n",
      "vehicle_theft\n",
      "burglary\n"
     ]
    }
   ],
   "source": [
    "for i in [10, 17, 6, 26, 20, 16, 11, 25, 2, 3]:\n",
    "    print(features[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This are the most important features descriptions\n",
    "'''\n",
    "'garbage'\n",
    "'Population 3 years and over enrolled in school'\n",
    "'weapons_violation'\n",
    "'Employment/Population Ratio!!Estimate!!Population 16 years and over'\n",
    "'Population 25 years and over!!9th to 12th grade, no diploma'\n",
    "'Average household size'\n",
    "'sanitation'\n",
    "'Households. FOOD STAMPS/Supplemental Nutrition Assistance Program (SNAP)'\n",
    "'vehicle_theft'\n",
    "'burglary'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Predictions for 2019 considering data from 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_2017 = pd.read_csv('clean_data/complete_2017.csv')\n",
    "tracts = inputs_2017['tract'].reset_index()\n",
    "inputs_2017.drop(['tract', 'S0101_C01_001E'], axis=1, inplace=True)\n",
    "inputs_2017 = cl.standardrize(inputs_2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "proba_1 = pd.DataFrame(mod_split1.predict_proba(inputs_2017))[[1]]\n",
    "proba_2 = pd.DataFrame(mod_split2.predict_proba(inputs_2017))[[1]]\n",
    "proba_3 = pd.DataFrame(mod_split3.predict_proba(inputs_2017))[[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average predict proba \n",
    "proba_avg = cl.average_df([proba_1, proba_2, proba_3], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>probability</th>\n",
       "      <th>tract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.996667</td>\n",
       "      <td>290900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.996667</td>\n",
       "      <td>671500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.996667</td>\n",
       "      <td>250800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.995000</td>\n",
       "      <td>691500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.995000</td>\n",
       "      <td>711000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   probability   tract\n",
       "0     0.996667  290900\n",
       "1     0.996667  671500\n",
       "2     0.996667  250800\n",
       "3     0.995000  691500\n",
       "4     0.995000  711000"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Average predict proba by tract\n",
    "probs_2019 = proba_avg.reset_index()\n",
    "probs_2019 = probs_2019.merge(tracts, on='index')\n",
    "probs_2019.drop('index', axis=1, inplace = True)\n",
    "probs_2019.columns = ['probability', 'tract']\n",
    "probs_2019.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building a Geopandas data frame to create a map\n",
    "tracts_geo = gpd.read_file('raw_data\\geo_export_fe9f2155-ba22-4697-91ff-daeee48c8d0b.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracts_geo = tracts_geo[['tractce10', 'geometry']]\n",
    "tracts_geo['tractce10'] = tracts_geo['tractce10'].astype(int)\n",
    "abandon_2019_pred = tracts_geo.merge(probs_2019, left_on='tractce10', right_on='tract', how='right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "abandon_2019_pred.to_file('clean_data/abandon_2019_pred.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['buss_licences', 'building_violations', 'vehicle_theft', 'burglary',\n",
       "       'robbery', 'public_peace_violation', 'weapons_violation',\n",
       "       'sexual_assault', 'homicides', 'rodents', 'garbage', 'sanitation',\n",
       "       'abandon_vehicles', 'pot_holes', 'tree_trims', 'street_lights',\n",
       "       'S1101_C01_002E', 'S1401_C02_001E', 'S1401_C02_030E', 'S1501_C02_002E',\n",
       "       'S1501_C02_008E', 'S1601_C02_003E', 'S1701_C03_001E', 'S1810_C03_001E',\n",
       "       'S1901_C01_012E', 'S2201_C02_001E', 'S2301_C03_001E', 'S2701_C03_001E'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
